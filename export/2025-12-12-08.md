# soft
## Bridging the Gap Between Avalanche Relaxation and Yielding Rheology
- **Url**: http://arxiv.org/abs/2504.18382v2
- **Authors**: ['Leonardo Relmucao-Leiva', 'Carlos Villarroel', 'Gustavo Düring']
- **Abstrat**: The yielding transition in amorphous materials, whether driven passively (simple shear) or actively, remains a fundamental open question in soft matter physics. While avalanche statistics at the critical point have been extensively studied, the emergence of the dynamic regime at yielding and the steady-state flow properties remain poorly understood. In particular, the significant variability observed in flow curves across different systems lacks a clear explanation. We determine, for the first time, the relationship between avalanche duration and size across the yielding transition, revealing how it evolves from quasistatic to dynamic flow regimes. This precise measurement is made using the Controlled Relaxation Time Model (CRTM), a new simulation framework that treats the relaxation time as a tunable parameter. CRTM reproduces known results in both limits and enables a direct analysis of the change of regime between them. Applying the model to different microscopic dynamics, we find that the existing scaling relation connecting critical exponents under flow holds for passive systems. However, active systems exhibit significant deviations, suggesting a missing ingredient in the current understanding of yielding.





## Nonlocal form factor of chromomagnetic penguin in $B\to K\ell^+\ell^-$ from QCD light-cone sum rules
- **Url**: http://arxiv.org/abs/2512.10868v1
- **Authors**: ['T. Hurth', 'A. Khodjamirian', 'F. Mahmoudi', 'D. Mishra', 'Y. Monceaux', 'S. Neshatpour']
- **Abstrat**: The branching fraction of the $B \to K\ell^+\ell^-$ decay has been measured recently by the LHC experiments, showing a deviation from theory predictions based on the Standard Model (SM). A major challenge in achieving a complete SM prediction and interpreting this discrepancy lies in the treatment of nonlocal hadronic effects. In $B \to K\ell^+\ell^-$, these effects are cast in a single nonlocal form factor, a function of squared momentum transfer $q^2$ to the lepton pair. One of the previously used methods provides this form factor in the region of spacelike momentum transfer, $q^2<0$, matching the result to the hadronic dispersion relation, which is then continued to the physical region. The calculation done so far was a combination of QCD factorisation for hard-gluon contributions with light-cone sum rules (LCSRs) for soft-gluon ones. In this work, we calculate for the first time the complete nonlocal form factor at $q^2<0$ for one of the effective operators, the chromomagnetic operator $O_{8g}$, applying the method of LCSRs with $ B$-meson distribution amplitudes. We compute, both analytically and numerically, the operator-product expansion (OPE) diagrams with hard-gluon exchanges, analyse their structure and hierarchy, and obtain their spectral density entering the LCSR together with soft-gluon contributions. This study paves the way for our next task, a complete calculation of nonlocal $B \to K\ell^+\ell^-$ form factor at spacelike $q^2$, including the dominant contributions of current-current operators, known as charm-loops.





## Exact Screening-Ranged Expansions for Many-Body Electrostatics
- **Url**: http://arxiv.org/abs/2512.09421v2
- **Authors**: ['Sergii V. Siryk', 'Walter Rocchia']
- **Abstrat**: We present an exact many-body framework for electrostatic interactions among $N$ arbitrarily charged spheres in an electrolyte, modeled by the linearized Poisson--Boltzmann equation. Building on a spectral analysis of nonstandard Neumann--Poincaré-type operators introduced in a companion mathematical work arXiv:2512.08684, we construct convergent screening-ranged series for the potential, interaction energy, and forces, where each term is associated with a well-defined Debye--Hückel screening order and can be obtained evaluating an analytical expression rather than numerically solving an infinitely dimensional linear system. This formulation unifies and extends classical and recent approaches, providing a rigorous basis for electrostatic interactions among heterogeneously charged particles (including Janus colloids) and yielding many-body generalizations of analytical explicit-form results previously available only for two-body systems. The framework captures and clarifies complex effects such as asymmetric dielectric screening, opposite-charge repulsion, and like-charge attraction, which remain largely analytically elusive in existing treatments. Beyond its fundamental significance, the method leads to numerically efficient schemes, offering a versatile tool for modeling colloids and soft/biological matter in electrolytic solution.





## V-OCBF: Learning Safety Filters from Offline Data via Value-Guided Offline Control Barrier Functions
- **Url**: http://arxiv.org/abs/2512.10822v1
- **Authors**: ['Mumuksh Tayal', 'Manan Tayal', 'Aditya Singh', 'Shishir Kolathaya', 'Ravi Prakash']
- **Abstrat**: Ensuring safety in autonomous systems requires controllers that satisfy hard, state-wise constraints without relying on online interaction. While existing Safe Offline RL methods typically enforce soft expected-cost constraints, they do not guarantee forward invariance. Conversely, Control Barrier Functions (CBFs) provide rigorous safety guarantees but usually depend on expert-designed barrier functions or full knowledge of the system dynamics. We introduce Value-Guided Offline Control Barrier Functions (V-OCBF), a framework that learns a neural CBF entirely from offline demonstrations. Unlike prior approaches, V-OCBF does not assume access to the dynamics model; instead, it derives a recursive finite-difference barrier update, enabling model-free learning of a barrier that propagates safety information over time. Moreover, V-OCBF incorporates an expectile-based objective that avoids querying the barrier on out-of-distribution actions and restricts updates to the dataset-supported action set. The learned barrier is then used with a Quadratic Program (QP) formulation to synthesize real-time safe control. Across multiple case studies, V-OCBF yields substantially fewer safety violations than baseline methods while maintaining strong task performance, highlighting its scalability for offline synthesis of safety-critical controllers without online interaction or hand-engineered barriers.





## Approximate N$^2$LO and N$^3$LO QCD Predictions for $tW$ Production
- **Url**: http://arxiv.org/abs/2512.10711v1
- **Authors**: ['Jia-Le Ding', 'Hai Tao Li', 'Jian Wang']
- **Abstrat**: We report a calculation of approximate next-to-next-to-leading-order (N$^2$LO) and next-to-N$^2$LO (N$^3$LO) QCD corrections to associated $tW$ production at the LHC, which constitute the dominant contributions to full perturbative predictions. The approximate N$^2$LO corrections consist of the large logarithmic terms $\ln^n (1-Q^2/\hat{s})$ (with $\sqrt{\hat{s}}$ being the partonic center-of-mass energy and $Q$ the invariant mass of the $tW$ system) and the terms proportional to $δ(1-Q^2/\hat{s})$ at $\mathcal{O}(α_s^2)$, which are obtained by utilizing the newly obtained two-loop hard and soft functions. The approximate N$^3$LO corrections further include the large logarithms at $\mathcal{O}(α_s^3)$ by using renormalization group evolution equations and the three-loop soft anomalous dimension, while the $δ(1-Q^2/\hat{s})$ term is only partially accurate at this order. Numerical evaluation reveals that they increase the NLO cross section by more than $10\%$. The inclusion of these higher-order corrections leads to improved agreement with the experimental data at the LHC, resulting in a direct determination of the CKM matrix element $|V_{tb}|=0.99\pm 0.03({\rm exp.})\pm 0.03({\rm theo.})$ without assuming unitarity of the matrix.





## Bio-Organic Materials Based Resistive Switching Memories
- **Url**: http://arxiv.org/abs/2512.10523v1
- **Authors**: ['Rahul Deb', 'Debajyoti Bhattacharjee', 'Syed Arshad Hussain']
- **Abstrat**: Resistive switching (RS) devices, based on soft materials such as organic, biomolecules as well as natural plant extracts etc., has emerged as a promising alternative to the conventional memory technologies. They offer simple device structures, low power requirements, rapid switching and compatibility with high-density device integration. Over the last two decades, these classes of materials have been explored for both non-volatile memory and artificial synapse functions. This chapter provides a brief overview of RS fundamentals, their major classifications, key applications, and recent trends in the use of organic and bio-derived materials.





## Surface-Polyconvex Models for Soft Elastic Solids
- **Url**: http://arxiv.org/abs/2503.24294v3
- **Authors**: ['Martin Horák', 'Michal Šmejkal', 'Martin Kružík']
- **Abstrat**: Soft solids with surface energy exhibit complex mechanical behavior, necessitating advanced constitutive models to capture the interplay between bulk and surface mechanics. This interplay has profound implications for material design and emerging technologies. In this work, we set up variational models for bulk-surface elasticity and explore a novel class of surface-polyconvex constitutive models that account for surface energy while ensuring the existence of minimizers. These models are implemented within a finite element framework and validated through benchmark problems and applications, including, e.g., the liquid bridge problem and the Rayleigh-Plateau instability, for which the surface energy plays the dominant role. The results demonstrate the ability of surface-polyconvex models to accurately capture surface-driven phenomena, establishing them as a powerful tool for advancing the mechanics of soft materials in both engineering and biological applications.





## A Physics-Informed Neural Network Framework for Simulating Creep Buckling in Growing Viscoelastic Biological Tissues
- **Url**: http://arxiv.org/abs/2506.18565v2
- **Authors**: ['Zhongya Lin', 'Jinshuai Bai', 'Shuang Li', 'Xindong Chen', 'Bo Li', 'Xi-Qiao Feng']
- **Abstrat**: Modeling viscoelastic behavior is crucial in engineering and biomechanics, where materials undergo time-dependent deformations, including stress relaxation, creep buckling and biological tissue development. Traditional numerical methods, like the finite element method, often require explicit meshing, artificial perturbations or embedding customised programs to capture these phenomena, adding computational complexity. In this study, we develop an energy-based physics-informed neural network (PINN) framework using an incremental approach to model viscoelastic creep, stress relaxation, buckling, and growth-induced morphogenesis. Physics consistency is ensured by training neural networks to minimize the systems potential energy functional, implicitly satisfying equilibrium and constitutive laws. We demonstrate that this framework can naturally capture creep buckling without pre-imposed imperfections, leveraging inherent training dynamics to trigger instabilities. Furthermore, we extend our framework to biological tissue growth and morphogenesis, predicting both uniform expansion and differential growth-induced buckling in cylindrical structures. Results show that the energy-based PINN effectively predicts viscoelastic instabilities, post-buckling evolution and tissue morphological evolution, offering a promising alternative to traditional methods. This study demonstrates that PINN can be a flexible robust tool for modeling complex, time-dependent material behavior, opening possible applications in structural engineering, soft materials, and tissue development.





## ShapeForce: Low-Cost Soft Robotic Wrist for Contact-Rich Manipulation
- **Url**: http://arxiv.org/abs/2511.19955v2
- **Authors**: ['Jinxuan Zhu', 'Zihao Yan', 'Yangyu Xiao', 'Jingxiang Guo', 'Chenrui Tie', 'Xinyi Cao', 'Yuhang Zheng', 'Lin Shao']
- **Abstrat**: Contact feedback is essential for contact-rich robotic manipulation, as it allows the robot to detect subtle interaction changes and adjust its actions accordingly. Six-axis force-torque sensors are commonly used to obtain contact feedback, but their high cost and fragility have discouraged many researchers from adopting them in contact-rich tasks. To offer a more cost-efficient and easy-accessible source of contact feedback, we present ShapeForce, a low-cost, plug-and-play soft wrist that provides force-like signals for contact-rich robotic manipulation. Inspired by how humans rely on relative force changes in contact rather than precise force magnitudes, ShapeForce converts external force and torque into measurable deformations of its compliant core, which are then estimated via marker-based pose tracking and converted into force-like signals. Our design eliminates the need for calibration or specialized electronics to obtain exact values, and instead focuses on capturing force and torque changes sufficient for enabling contact-rich manipulation. Extensive experiments across diverse contact-rich tasks and manipulation policies demonstrate that ShapeForce delivers performance comparable to six-axis force-torque sensors at an extremely low cost.





## Deformation and organization of droplet-encapsulated soft beads
- **Url**: http://arxiv.org/abs/2511.18002v2
- **Authors**: ['Shunsuke Saita', 'Finn Bastian Molzahn', 'Clara Delahousse', 'Julien Husson', 'Charles N. Baroud']
- **Abstrat**: Many biological, culinary, and engineering processes lead to the co-encapsulation of several soft particles within a liquid interface. In these situations the particles are bound together by the capillary forces that deform them and influence their biological or rheological properties. Here we introduce an experimental approach to encapsulate a controlled number of soft beads within aqueous droplets in oil. These droplet-encapsulated gels are manipulated in a deformable microfluidic device to merge them and modify the liquid fraction. In the dry limit the contact surface between the hydrogels is found to be determined by the elastocapillary number $E_c$, with the contact radius scaling as $E_c^{1/3}$, indicating that the deformation increases for soft or small particles. When multiple beads are co-encapsulated within a single droplet they can be arranged into linear or three-dimensional aggregates that remain at a local energy minimum.





# robot
## Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision
- **Url**: http://arxiv.org/abs/2512.10956v1
- **Authors**: ['Wentao Zhou', 'Xuweiyi Chen', 'Vignesh Rajagopal', 'Jeffrey Chen', 'Rohan Chandra', 'Zezhou Cheng']
- **Abstrat**: The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.   We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.





## Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation
- **Url**: http://arxiv.org/abs/2512.10925v1
- **Authors**: ['Zamirddine Mari', 'Mohamad Motasem Nawaf', 'Pierre Drap']
- **Abstrat**: Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.





## Iterative Compositional Data Generation for Robot Control
- **Url**: http://arxiv.org/abs/2512.10891v1
- **Authors**: ['Anh-Quan Pham', 'Marcel Hussing', 'Shubhankar P. Patankar', 'Dani S. Bassett', 'Jorge Mendez-Mendez', 'Eric Eaton']
- **Abstrat**: Collecting robotic manipulation data is expensive, making it impractical to acquire demonstrations for the combinatorially large space of tasks that arise in multi-object, multi-robot, and multi-environment settings. While recent generative models can synthesize useful data for individual tasks, they do not exploit the compositional structure of robotic domains and struggle to generalize to unseen task combinations. We propose a semantic compositional diffusion transformer that factorizes transitions into robot-, object-, obstacle-, and objective-specific components and learns their interactions through attention. Once trained on a limited subset of tasks, we show that our model can zero-shot generate high-quality transitions from which we can learn control policies for unseen task combinations. Then, we introduce an iterative self-improvement procedure in which synthetic data is validated via offline reinforcement learning and incorporated into subsequent training rounds. Our approach substantially improves zero-shot performance over monolithic and hard-coded compositional baselines, ultimately solving nearly all held-out tasks and demonstrating the emergence of meaningful compositional structure in the learned representations.





## MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence
- **Url**: http://arxiv.org/abs/2512.10863v1
- **Authors**: ['Jingli Lin', 'Runsen Xu', 'Shaohao Zhu', 'Sihan Yang', 'Peizhou Cao', 'Yunlong Ran', 'Miao Hu', 'Chenming Zhu', 'Yiman Xie', 'Yilin Long', 'Wenbo Hu', 'Dahua Lin', 'Tai Wang', 'Jiangmiao Pang']
- **Abstrat**: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.





## From Generated Human Videos to Physically Plausible Robot Trajectories
- **Url**: http://arxiv.org/abs/2512.05094v2
- **Authors**: ['James Ni', 'Zekai Wang', 'Wei Lin', 'Amir Bar', 'Yann LeCun', 'Trevor Darrell', 'Jitendra Malik', 'Roei Herzig']
- **Abstrat**: Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.





## Reframing Human-Robot Interaction Through Extended Reality: Unlocking Safer, Smarter, and More Empathic Interactions with Virtual Robots and Foundation Models
- **Url**: http://arxiv.org/abs/2512.02569v2
- **Authors**: ['Yuchong Zhang', 'Yong Ma', 'Danica Kragic']
- **Abstrat**: This perspective reframes human-robot interaction (HRI) through extended reality (XR), arguing that virtual robots powered by large foundation models (FMs) can serve as cognitively grounded, empathic agents. Unlike physical robots, XR-native agents are unbound by hardware constraints and can be instantiated, adapted, and scaled on demand, while still affording embodiment and co-presence. We synthesize work across XR, HRI, and cognitive AI to show how such agents can support safety-critical scenarios, socially and cognitively empathic interaction across domains, and outreaching physical capabilities with XR and AI integration. We then discuss how multimodal large FMs (e.g., large language model, large vision model, and vision-language model) enable context-aware reasoning, affect-sensitive situations, and long-term adaptation, positioning virtual robots as cognitive and empathic mediators rather than mere simulation assets. At the same time, we highlight challenges and potential risks, including overtrust, cultural and representational bias, privacy concerns around biometric sensing, and data governance and transparency. The paper concludes by outlining a research agenda for human-centered, ethically grounded XR agents - emphasizing multi-layered evaluation frameworks, multi-user ecosystems, mixed virtual-physical embodiment, and societal and ethical design practices to envision XR-based virtual agents powered by FMs as reshaping future HRI into a more efficient and adaptive paradigm.





## Panoramic Out-of-Distribution Segmentation
- **Url**: http://arxiv.org/abs/2505.03539v3
- **Authors**: ['Mengfei Duan', 'Yuheng Zhang', 'Yihong Cao', 'Fei Teng', 'Kai Luo', 'Jiaming Zhang', 'Kailun Yang', 'Zhiyong Li']
- **Abstrat**: Panoramic imaging enables capturing 360° images with an ultra-wide Field-of-View (FoV) for dense omnidirectional perception, which is critical to applications, such as autonomous driving and augmented reality, etc. However, current panoramic semantic segmentation methods fail to identify outliers, and pinhole Out-of-distribution Segmentation (OoS) models perform unsatisfactorily in the panoramic domain due to pixel distortions and background clutter. To address these issues, we introduce a new task, Panoramic Out-of-distribution Segmentation (PanOoS), with the aim of achieving comprehensive and safe scene understanding. Furthermore, we propose the first solution, POS, which adapts to the characteristics of panoramic images through text-guided prompt distribution learning. Specifically, POS integrates a disentanglement strategy designed to materialize the cross-domain generalization capability of CLIP. The proposed Prompt-based Restoration Attention (PRA) optimizes semantic decoding by prompt guidance and self-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL) refines the manifold of per-pixel mask embeddings via semantic prototype supervision. Besides, to compensate for the scarcity of PanOoS datasets, we establish two benchmarks: DenseOoS, which features diverse outliers in complex environments, and QuadOoS, captured by a quadruped robot with a panoramic annular lens system. Extensive experiments demonstrate superior performance of POS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS, outperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves leading closed-set segmentation capabilities and advances the development of panoramic understanding. Code and datasets will be available at https://github.com/MengfeiD/PanOoS.





## Evaluating Gemini Robotics Policies in a Veo World Simulator
- **Url**: http://arxiv.org/abs/2512.10675v1
- **Authors**: ['Gemini Robotics Team', 'Coline Devin', 'Yilun Du', 'Debidatta Dwibedi', 'Ruiqi Gao', 'Abhishek Jindal', 'Thomas Kipf', 'Sean Kirmani', 'Fangchen Liu', 'Anirudha Majumdar', 'Andrew Marmon', 'Carolina Parada', 'Yulia Rubanova', 'Dhruv Shah', 'Vikas Sindhwani', 'Jie Tan', 'Fei Xia', 'Ted Xiao', 'Sherry Yang', 'Wenhao Yu', 'Allan Zhou']
- **Abstrat**: Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.





## Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching
- **Url**: http://arxiv.org/abs/2512.10674v1
- **Authors**: ['Javier Villena Toro', 'Mehdi Tarkian']
- **Abstrat**: Recent progress in zero-shot 6D object pose estimation has been driven largely by large-scale models and cloud-based inference. However, these approaches often introduce high latency, elevated energy consumption, and deployment risks related to connectivity, cost, and data governance; factors that conflict with the practical constraints of real-world robotics, where compute is limited and on-device inference is frequently required. We introduce Geo6DPose, a lightweight, fully local, and training-free pipeline for zero-shot 6D pose estimation that trades model scale for geometric reliability. Our method combines foundation model visual features with a geometric filtering strategy: Similarity maps are computed between onboarded template DINO descriptors and scene patches, and mutual correspondences are established by projecting scene patch centers to 3D and template descriptors to the object model coordinate system. Final poses are recovered via correspondence-driven RANSAC and ranked using a weighted geometric alignment metric that jointly accounts for reprojection consistency and spatial support, improving robustness to noise, clutter, and partial visibility. Geo6DPose achieves sub-second inference on a single commodity GPU while matching the average recall of significantly larger zero-shot baselines (53.7 AR, 1.08 FPS). It requires no training, fine-tuning, or network access, and remains compatible with evolving foundation backbones, advancing practical, fully local 6D perception for robotic deployment.





## XDen-1K: A Density Field Dataset of Real-World Objects
- **Url**: http://arxiv.org/abs/2512.10668v1
- **Authors**: ['Jingxuan Zhang', 'Tianqi Yu', 'Yatu Zhang', 'Jinze Wu', 'Kaixin Yao', 'Jingyang Liu', 'Yuyao Zhang', 'Jiayuan Gu', 'Jingyi Yu']
- **Abstrat**: A deep understanding of the physical world is a central goal for embodied AI and realistic simulation. While current models excel at capturing an object's surface geometry and appearance, they largely neglect its internal physical properties. This omission is critical, as properties like volumetric density are fundamental for predicting an object's center of mass, stability, and interaction dynamics in applications ranging from robotic manipulation to physical simulation. The primary bottleneck has been the absence of large-scale, real-world data. To bridge this gap, we introduce XDen-1K, the first large-scale, multi-modal dataset designed for real-world physical property estimation, with a particular focus on volumetric density. The core of this dataset consists of 1,000 real-world objects across 148 categories, for which we provide comprehensive multi-modal data, including a high-resolution 3D geometric model with part-level annotations and a corresponding set of real-world biplanar X-ray scans. Building upon this data, we introduce a novel optimization framework that recovers a high-fidelity volumetric density field of each object from its sparse X-ray views. To demonstrate its practical value, we add X-ray images as a conditioning signal to an existing segmentation network and perform volumetric segmentation. Furthermore, we conduct experiments on downstream robotics tasks. The results show that leveraging the dataset can effectively improve the accuracy of center-of-mass estimation and the success rate of robotic manipulation. We believe XDen-1K will serve as a foundational resource and a challenging new benchmark, catalyzing future research in physically grounded visual inference and embodied AI.





## K-Track: Kalman-Enhanced Tracking for Accelerating Deep Point Trackers on Edge Devices
- **Url**: http://arxiv.org/abs/2512.10628v1
- **Authors**: ['Bishoy Galoaa', 'Pau Closas', 'Sarah Ostadabbas']
- **Abstrat**: Point tracking in video sequences is a foundational capability for real-world computer vision applications, including robotics, autonomous systems, augmented reality, and video analysis. While recent deep learning-based trackers achieve state-of-the-art accuracy on challenging benchmarks, their reliance on per-frame GPU inference poses a major barrier to deployment on resource-constrained edge devices, where compute, power, and connectivity are limited. We introduce K-Track (Kalman-enhanced Tracking), a general-purpose, tracker-agnostic acceleration framework designed to bridge this deployment gap. K-Track reduces inference cost by combining sparse deep learning keyframe updates with lightweight Kalman filtering for intermediate frame prediction, using principled Bayesian uncertainty propagation to maintain temporal coherence. This hybrid strategy enables 5-10X speedup while retaining over 85% of the original trackers' accuracy. We evaluate K-Track across multiple state-of-the-art point trackers and demonstrate real-time performance on edge platforms such as the NVIDIA Jetson Nano and RTX Titan. By preserving accuracy while dramatically lowering computational requirements, K-Track provides a practical path toward deploying high-quality point tracking in real-world, resource-limited settings, closing the gap between modern tracking algorithms and deployable vision systems.





## LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator
- **Url**: http://arxiv.org/abs/2512.10605v1
- **Authors**: ['Lihuang Chen', 'Xiangyu Luo', 'Jun Meng']
- **Abstrat**: We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent.





## Towards Open-World Human Action Segmentation Using Graph Convolutional Networks
- **Url**: http://arxiv.org/abs/2507.00756v2
- **Authors**: ['Hao Xing', 'Kai Zhe Boey', 'Gordon Cheng']
- **Abstrat**: Human-object interaction segmentation is a fundamental task of daily activity understanding, which plays a crucial role in applications such as assistive robotics, healthcare, and autonomous systems. Most existing learning-based methods excel in closed-world action segmentation, they struggle to generalize to open-world scenarios where novel actions emerge. Collecting exhaustive action categories for training is impractical due to the dynamic diversity of human activities, necessitating models that detect and segment out-of-distribution actions without manual annotation. To address this issue, we formally define the open-world action segmentation problem and propose a structured framework for detecting and segmenting unseen actions. Our framework introduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional Network (EPGCN) with a novel decoder module for robust spatiotemporal feature upsampling. 2) Mixup-based training to synthesize out-of-distribution data, eliminating reliance on manual annotations. 3) A novel Temporal Clustering loss that groups in-distribution actions while distancing out-of-distribution samples.   We evaluate our framework on two challenging human-object interaction recognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets. Experimental results demonstrate significant improvements over state-of-the-art action segmentation models across multiple open-set evaluation metrics, achieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and out-of-distribution detection performances (AUROC), respectively. Additionally, we conduct an in-depth ablation study to assess the impact of each proposed component, identifying the optimal framework configuration for open-world action segmentation.





## Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation
- **Url**: http://arxiv.org/abs/2507.00752v2
- **Authors**: ['Hao Xing', 'Kai Zhe Boey', 'Yuankai Wu', 'Darius Burschka', 'Gordon Cheng']
- **Abstrat**: Accurate temporal segmentation of human actions is critical for intelligent robots in collaborative settings, where a precise understanding of sub-activity labels and their temporal structure is essential. However, the inherent noise in both human pose estimation and object detection often leads to over-segmentation errors, disrupting the coherence of action sequences. To address this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that integrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g., 30 fps) motion data (skeleton and object detections) to mitigate fragmentation. Our framework introduces three key contributions. First, a sinusoidal encoding strategy that maps 3D skeleton coordinates into a continuous sin-cos space to enhance spatial representation robustness. Second, a temporal graph fusion module that aligns multi-modal inputs with differing resolutions via hierarchical feature aggregation, Third, inspired by the smooth transitions inherent to human actions, we design SmoothLabelMix, a data augmentation technique that mixes input sequences and labels to generate synthetic training examples with gradual action transitions, enhancing temporal consistency in predictions and reducing over-segmentation artifacts.   Extensive experiments on the Bimanual Actions Dataset, a public benchmark for human-object interaction understanding, demonstrate that our approach outperforms state-of-the-art methods, especially in action segmentation accuracy, achieving F1@10: 94.5% and F1@25: 92.8%.





## Mr. Virgil: Learning Multi-robot Visual-range Relative Localization
- **Url**: http://arxiv.org/abs/2512.10540v1
- **Authors**: ['Si Wang', 'Zhehan Li', 'Jiadong Lu', 'Rong Xiong', 'Yanjun Cao', 'Yue Wang']
- **Abstrat**: Ultra-wideband (UWB)-vision fusion localization has achieved extensive applications in the domain of multi-agent relative localization. The challenging matching problem between robots and visual detection renders existing methods highly dependent on identity-encoded hardware or delicate tuning algorithms. Overconfident yet erroneous matches may bring about irreversible damage to the localization system. To address this issue, we introduce Mr. Virgil, an end-to-end learning multi-robot visual-range relative localization framework, consisting of a graph neural network for data association between UWB rangings and visual detections, and a differentiable pose graph optimization (PGO) back-end. The graph-based front-end supplies robust matching results, accurate initial position predictions, and credible uncertainty estimates, which are subsequently integrated into the PGO back-end to elevate the accuracy of the final pose estimation. Additionally, a decentralized system is implemented for real-world applications. Experiments spanning varying robot numbers, simulation and real-world, occlusion and non-occlusion conditions showcase the stability and exactitude under various scenes compared to conventional methods. Our code is available at: https://github.com/HiOnes/Mr-Virgil.





## Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning
- **Url**: http://arxiv.org/abs/2511.14396v2
- **Authors**: ['Xiuxiu Qi', 'Yu Yang', 'Jiannong Cao', 'Luyao Bai', 'Chongshan Fan', 'Chengtai Cao', 'Hongpeng Wang']
- **Abstrat**: Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.





## Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections
- **Url**: http://arxiv.org/abs/2506.16685v4
- **Authors**: ['Xiaomeng Xu', 'Yifan Hou', 'Zeyi Liu', 'Shuran Song']
- **Abstrat**: We address key challenges in Dataset Aggregation (DAgger) for real-world contact-rich manipulation: how to collect informative human correction data and how to effectively update policies with this new data. We introduce Compliant Residual DAgger (CR-DAgger), which contains two novel components: 1) a Compliant Intervention Interface that leverages compliance control, allowing humans to provide gentle, accurate delta action corrections without interrupting the ongoing robot policy execution; and 2) a Compliant Residual Policy formulation that learns from human corrections while incorporating force feedback and force control. Our system significantly enhances performance on precise contact-rich manipulation tasks using minimal correction data, improving base policy success rates by over 50\% on two challenging tasks (book flipping and belt assembly) while outperforming both retraining-from-scratch and finetuning approaches. Through extensive real-world experiments, we provide practical guidance for implementing effective DAgger in real-world robot learning tasks. Result videos are available at: https://compliant-residual-dagger.github.io/





## UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning
- **Url**: http://arxiv.org/abs/2512.10492v1
- **Authors**: ['Jiaxi Wu', 'Tiantian Zhang', 'Yuxing Wang', 'Yongzhe Chang', 'Xueqian Wang']
- **Abstrat**: Robust adversarial reinforcement learning has emerged as an effective paradigm for training agents to handle uncertain disturbance in real environments, with critical applications in sequential decision-making domains such as autonomous driving and robotic control. Within this paradigm, agent training is typically formulated as a zero-sum Markov game between a protagonist and an adversary to enhance policy robustness. However, the trainable nature of the adversary inevitably induces non-stationarity in the learning dynamics, leading to exacerbated training instability and convergence difficulties, particularly in high-dimensional complex environments. In this paper, we propose a novel approach, Uncertainty-Aware Critic Ensemble for robust adversarial Reinforcement learning (UACER), which consists of two strategies: 1) Diversified critic ensemble: a diverse set of K critic networks is exploited in parallel to stabilize Q-value estimation rather than conventional single-critic architectures for both variance reduction and robustness enhancement. 2) Time-varying Decay Uncertainty (TDU) mechanism: advancing beyond simple linear combinations, we develop a variance-derived Q-value aggregation strategy that explicitly incorporates epistemic uncertainty to dynamically regulate the exploration-exploitation trade-off while simultaneously stabilizing the training process. Comprehensive experiments across several MuJoCo control problems validate the superior effectiveness of UACER, outperforming state-of-the-art methods in terms of overall performance, stability, and efficiency.





## Contact SLAM: An Active Tactile Exploration Policy Based on Physical Reasoning Utilized in Robotic Fine Blind Manipulation Tasks
- **Url**: http://arxiv.org/abs/2512.10481v1
- **Authors**: ['Gaozhao Wang', 'Xing Liu', 'Zhenduo Ye', 'Zhengxiong Liu', 'Panfeng Huang']
- **Abstrat**: Contact-rich manipulation is difficult for robots to execute and requires accurate perception of the environment. In some scenarios, vision is occluded. The robot can then no longer obtain real-time scene state information through visual feedback. This is called ``blind manipulation". In this manuscript, a novel physically-driven contact cognition method, called ``Contact SLAM", is proposed. It estimates the state of the environment and achieves manipulation using only tactile sensing and prior knowledge of the scene. To maximize exploration efficiency, this manuscript also designs an active exploration policy. The policy gradually reduces uncertainties in the manipulation scene. The experimental results demonstrated the effectiveness and accuracy of the proposed method in several contact-rich tasks, including the difficult and delicate socket assembly task and block-pushing task.





## Symphony: A Heuristic Normalized Calibrated Advantage Actor and Critic Algorithm in application for Humanoid Robots
- **Url**: http://arxiv.org/abs/2512.10477v1
- **Authors**: ['Timur Ishuov', 'Michele Folgheraiter', 'Madi Nurmanov', 'Goncalo Gordo', 'Richárd Farkas', 'József Dombi']
- **Abstrat**: In our work we not explicitly hint that it is a misconception to think that humans learn fast. Learning process takes time. Babies start learning to move in the restricted liquid area called placenta. Children often are limited by underdeveloped body. Even adults are not allowed to participate in complex competitions right away. However, with robots, when learning from scratch, we often don't have the privilege of waiting for dozen millions of steps. "Swaddling" regularization is responsible for restraining an agent in rapid but unstable development penalizing action strength in a specific way not affecting actions directly. The Symphony, Transitional-policy Deterministic Actor and Critic algorithm, is a concise combination of different ideas for possibility of training humanoid robots from scratch with Sample Efficiency, Sample Proximity and Safety of Actions in mind. It is no secret that continuous increase in Gaussian noise without appropriate smoothing is harmful for motors and gearboxes. Compared to Stochastic algorithms, we set a limited parametric noise and promote a reduced strength of actions, safely increasing entropy, since the actions are kind of immersed in weaker noise. When actions require more extreme values, actions rise above the weak noise. Training becomes empirically much safer for both the environment around and the robot's mechanisms. We use Fading Replay Buffer: using a fixed formula containing the hyperbolic tangent, we adjust the batch sampling probability: the memory contains a recent memory and a long-term memory trail. Fading Replay Buffer allows us to use Temporal Advantage when we improve the current Critic Network prediction compared to the exponential moving average. Temporal Advantage allows us to update Actor and Critic in one pass, as well as combine Actor and Critic in one Object and implement their Losses in one line.




# soft
## Bridging the Gap Between Avalanche Relaxation and Yielding Rheology
- **Url**: http://arxiv.org/abs/2504.18382v2
- **Authors**: ['Leonardo Relmucao-Leiva', 'Carlos Villarroel', 'Gustavo Düring']
- **Abstrat**: The yielding transition in amorphous materials, whether driven passively (simple shear) or actively, remains a fundamental open question in soft matter physics. While avalanche statistics at the critical point have been extensively studied, the emergence of the dynamic regime at yielding and the steady-state flow properties remain poorly understood. In particular, the significant variability observed in flow curves across different systems lacks a clear explanation. We determine, for the first time, the relationship between avalanche duration and size across the yielding transition, revealing how it evolves from quasistatic to dynamic flow regimes. This precise measurement is made using the Controlled Relaxation Time Model (CRTM), a new simulation framework that treats the relaxation time as a tunable parameter. CRTM reproduces known results in both limits and enables a direct analysis of the change of regime between them. Applying the model to different microscopic dynamics, we find that the existing scaling relation connecting critical exponents under flow holds for passive systems. However, active systems exhibit significant deviations, suggesting a missing ingredient in the current understanding of yielding.





## Nonlocal form factor of chromomagnetic penguin in $B\to K\ell^+\ell^-$ from QCD light-cone sum rules
- **Url**: http://arxiv.org/abs/2512.10868v1
- **Authors**: ['T. Hurth', 'A. Khodjamirian', 'F. Mahmoudi', 'D. Mishra', 'Y. Monceaux', 'S. Neshatpour']
- **Abstrat**: The branching fraction of the $B \to K\ell^+\ell^-$ decay has been measured recently by the LHC experiments, showing a deviation from theory predictions based on the Standard Model (SM). A major challenge in achieving a complete SM prediction and interpreting this discrepancy lies in the treatment of nonlocal hadronic effects. In $B \to K\ell^+\ell^-$, these effects are cast in a single nonlocal form factor, a function of squared momentum transfer $q^2$ to the lepton pair. One of the previously used methods provides this form factor in the region of spacelike momentum transfer, $q^2<0$, matching the result to the hadronic dispersion relation, which is then continued to the physical region. The calculation done so far was a combination of QCD factorisation for hard-gluon contributions with light-cone sum rules (LCSRs) for soft-gluon ones. In this work, we calculate for the first time the complete nonlocal form factor at $q^2<0$ for one of the effective operators, the chromomagnetic operator $O_{8g}$, applying the method of LCSRs with $ B$-meson distribution amplitudes. We compute, both analytically and numerically, the operator-product expansion (OPE) diagrams with hard-gluon exchanges, analyse their structure and hierarchy, and obtain their spectral density entering the LCSR together with soft-gluon contributions. This study paves the way for our next task, a complete calculation of nonlocal $B \to K\ell^+\ell^-$ form factor at spacelike $q^2$, including the dominant contributions of current-current operators, known as charm-loops.





## Exact Screening-Ranged Expansions for Many-Body Electrostatics
- **Url**: http://arxiv.org/abs/2512.09421v2
- **Authors**: ['Sergii V. Siryk', 'Walter Rocchia']
- **Abstrat**: We present an exact many-body framework for electrostatic interactions among $N$ arbitrarily charged spheres in an electrolyte, modeled by the linearized Poisson--Boltzmann equation. Building on a spectral analysis of nonstandard Neumann--Poincaré-type operators introduced in a companion mathematical work arXiv:2512.08684, we construct convergent screening-ranged series for the potential, interaction energy, and forces, where each term is associated with a well-defined Debye--Hückel screening order and can be obtained evaluating an analytical expression rather than numerically solving an infinitely dimensional linear system. This formulation unifies and extends classical and recent approaches, providing a rigorous basis for electrostatic interactions among heterogeneously charged particles (including Janus colloids) and yielding many-body generalizations of analytical explicit-form results previously available only for two-body systems. The framework captures and clarifies complex effects such as asymmetric dielectric screening, opposite-charge repulsion, and like-charge attraction, which remain largely analytically elusive in existing treatments. Beyond its fundamental significance, the method leads to numerically efficient schemes, offering a versatile tool for modeling colloids and soft/biological matter in electrolytic solution.





## V-OCBF: Learning Safety Filters from Offline Data via Value-Guided Offline Control Barrier Functions
- **Url**: http://arxiv.org/abs/2512.10822v1
- **Authors**: ['Mumuksh Tayal', 'Manan Tayal', 'Aditya Singh', 'Shishir Kolathaya', 'Ravi Prakash']
- **Abstrat**: Ensuring safety in autonomous systems requires controllers that satisfy hard, state-wise constraints without relying on online interaction. While existing Safe Offline RL methods typically enforce soft expected-cost constraints, they do not guarantee forward invariance. Conversely, Control Barrier Functions (CBFs) provide rigorous safety guarantees but usually depend on expert-designed barrier functions or full knowledge of the system dynamics. We introduce Value-Guided Offline Control Barrier Functions (V-OCBF), a framework that learns a neural CBF entirely from offline demonstrations. Unlike prior approaches, V-OCBF does not assume access to the dynamics model; instead, it derives a recursive finite-difference barrier update, enabling model-free learning of a barrier that propagates safety information over time. Moreover, V-OCBF incorporates an expectile-based objective that avoids querying the barrier on out-of-distribution actions and restricts updates to the dataset-supported action set. The learned barrier is then used with a Quadratic Program (QP) formulation to synthesize real-time safe control. Across multiple case studies, V-OCBF yields substantially fewer safety violations than baseline methods while maintaining strong task performance, highlighting its scalability for offline synthesis of safety-critical controllers without online interaction or hand-engineered barriers.





## Approximate N$^2$LO and N$^3$LO QCD Predictions for $tW$ Production
- **Url**: http://arxiv.org/abs/2512.10711v1
- **Authors**: ['Jia-Le Ding', 'Hai Tao Li', 'Jian Wang']
- **Abstrat**: We report a calculation of approximate next-to-next-to-leading-order (N$^2$LO) and next-to-N$^2$LO (N$^3$LO) QCD corrections to associated $tW$ production at the LHC, which constitute the dominant contributions to full perturbative predictions. The approximate N$^2$LO corrections consist of the large logarithmic terms $\ln^n (1-Q^2/\hat{s})$ (with $\sqrt{\hat{s}}$ being the partonic center-of-mass energy and $Q$ the invariant mass of the $tW$ system) and the terms proportional to $δ(1-Q^2/\hat{s})$ at $\mathcal{O}(α_s^2)$, which are obtained by utilizing the newly obtained two-loop hard and soft functions. The approximate N$^3$LO corrections further include the large logarithms at $\mathcal{O}(α_s^3)$ by using renormalization group evolution equations and the three-loop soft anomalous dimension, while the $δ(1-Q^2/\hat{s})$ term is only partially accurate at this order. Numerical evaluation reveals that they increase the NLO cross section by more than $10\%$. The inclusion of these higher-order corrections leads to improved agreement with the experimental data at the LHC, resulting in a direct determination of the CKM matrix element $|V_{tb}|=0.99\pm 0.03({\rm exp.})\pm 0.03({\rm theo.})$ without assuming unitarity of the matrix.





## Bio-Organic Materials Based Resistive Switching Memories
- **Url**: http://arxiv.org/abs/2512.10523v1
- **Authors**: ['Rahul Deb', 'Debajyoti Bhattacharjee', 'Syed Arshad Hussain']
- **Abstrat**: Resistive switching (RS) devices, based on soft materials such as organic, biomolecules as well as natural plant extracts etc., has emerged as a promising alternative to the conventional memory technologies. They offer simple device structures, low power requirements, rapid switching and compatibility with high-density device integration. Over the last two decades, these classes of materials have been explored for both non-volatile memory and artificial synapse functions. This chapter provides a brief overview of RS fundamentals, their major classifications, key applications, and recent trends in the use of organic and bio-derived materials.





## Surface-Polyconvex Models for Soft Elastic Solids
- **Url**: http://arxiv.org/abs/2503.24294v3
- **Authors**: ['Martin Horák', 'Michal Šmejkal', 'Martin Kružík']
- **Abstrat**: Soft solids with surface energy exhibit complex mechanical behavior, necessitating advanced constitutive models to capture the interplay between bulk and surface mechanics. This interplay has profound implications for material design and emerging technologies. In this work, we set up variational models for bulk-surface elasticity and explore a novel class of surface-polyconvex constitutive models that account for surface energy while ensuring the existence of minimizers. These models are implemented within a finite element framework and validated through benchmark problems and applications, including, e.g., the liquid bridge problem and the Rayleigh-Plateau instability, for which the surface energy plays the dominant role. The results demonstrate the ability of surface-polyconvex models to accurately capture surface-driven phenomena, establishing them as a powerful tool for advancing the mechanics of soft materials in both engineering and biological applications.





## A Physics-Informed Neural Network Framework for Simulating Creep Buckling in Growing Viscoelastic Biological Tissues
- **Url**: http://arxiv.org/abs/2506.18565v2
- **Authors**: ['Zhongya Lin', 'Jinshuai Bai', 'Shuang Li', 'Xindong Chen', 'Bo Li', 'Xi-Qiao Feng']
- **Abstrat**: Modeling viscoelastic behavior is crucial in engineering and biomechanics, where materials undergo time-dependent deformations, including stress relaxation, creep buckling and biological tissue development. Traditional numerical methods, like the finite element method, often require explicit meshing, artificial perturbations or embedding customised programs to capture these phenomena, adding computational complexity. In this study, we develop an energy-based physics-informed neural network (PINN) framework using an incremental approach to model viscoelastic creep, stress relaxation, buckling, and growth-induced morphogenesis. Physics consistency is ensured by training neural networks to minimize the systems potential energy functional, implicitly satisfying equilibrium and constitutive laws. We demonstrate that this framework can naturally capture creep buckling without pre-imposed imperfections, leveraging inherent training dynamics to trigger instabilities. Furthermore, we extend our framework to biological tissue growth and morphogenesis, predicting both uniform expansion and differential growth-induced buckling in cylindrical structures. Results show that the energy-based PINN effectively predicts viscoelastic instabilities, post-buckling evolution and tissue morphological evolution, offering a promising alternative to traditional methods. This study demonstrates that PINN can be a flexible robust tool for modeling complex, time-dependent material behavior, opening possible applications in structural engineering, soft materials, and tissue development.





## ShapeForce: Low-Cost Soft Robotic Wrist for Contact-Rich Manipulation
- **Url**: http://arxiv.org/abs/2511.19955v2
- **Authors**: ['Jinxuan Zhu', 'Zihao Yan', 'Yangyu Xiao', 'Jingxiang Guo', 'Chenrui Tie', 'Xinyi Cao', 'Yuhang Zheng', 'Lin Shao']
- **Abstrat**: Contact feedback is essential for contact-rich robotic manipulation, as it allows the robot to detect subtle interaction changes and adjust its actions accordingly. Six-axis force-torque sensors are commonly used to obtain contact feedback, but their high cost and fragility have discouraged many researchers from adopting them in contact-rich tasks. To offer a more cost-efficient and easy-accessible source of contact feedback, we present ShapeForce, a low-cost, plug-and-play soft wrist that provides force-like signals for contact-rich robotic manipulation. Inspired by how humans rely on relative force changes in contact rather than precise force magnitudes, ShapeForce converts external force and torque into measurable deformations of its compliant core, which are then estimated via marker-based pose tracking and converted into force-like signals. Our design eliminates the need for calibration or specialized electronics to obtain exact values, and instead focuses on capturing force and torque changes sufficient for enabling contact-rich manipulation. Extensive experiments across diverse contact-rich tasks and manipulation policies demonstrate that ShapeForce delivers performance comparable to six-axis force-torque sensors at an extremely low cost.





## Deformation and organization of droplet-encapsulated soft beads
- **Url**: http://arxiv.org/abs/2511.18002v2
- **Authors**: ['Shunsuke Saita', 'Finn Bastian Molzahn', 'Clara Delahousse', 'Julien Husson', 'Charles N. Baroud']
- **Abstrat**: Many biological, culinary, and engineering processes lead to the co-encapsulation of several soft particles within a liquid interface. In these situations the particles are bound together by the capillary forces that deform them and influence their biological or rheological properties. Here we introduce an experimental approach to encapsulate a controlled number of soft beads within aqueous droplets in oil. These droplet-encapsulated gels are manipulated in a deformable microfluidic device to merge them and modify the liquid fraction. In the dry limit the contact surface between the hydrogels is found to be determined by the elastocapillary number $E_c$, with the contact radius scaling as $E_c^{1/3}$, indicating that the deformation increases for soft or small particles. When multiple beads are co-encapsulated within a single droplet they can be arranged into linear or three-dimensional aggregates that remain at a local energy minimum.





# robot
## Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision
- **Url**: http://arxiv.org/abs/2512.10956v1
- **Authors**: ['Wentao Zhou', 'Xuweiyi Chen', 'Vignesh Rajagopal', 'Jeffrey Chen', 'Rohan Chandra', 'Zezhou Cheng']
- **Abstrat**: The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.   We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.





## Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation
- **Url**: http://arxiv.org/abs/2512.10925v1
- **Authors**: ['Zamirddine Mari', 'Mohamad Motasem Nawaf', 'Pierre Drap']
- **Abstrat**: Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.





## Iterative Compositional Data Generation for Robot Control
- **Url**: http://arxiv.org/abs/2512.10891v1
- **Authors**: ['Anh-Quan Pham', 'Marcel Hussing', 'Shubhankar P. Patankar', 'Dani S. Bassett', 'Jorge Mendez-Mendez', 'Eric Eaton']
- **Abstrat**: Collecting robotic manipulation data is expensive, making it impractical to acquire demonstrations for the combinatorially large space of tasks that arise in multi-object, multi-robot, and multi-environment settings. While recent generative models can synthesize useful data for individual tasks, they do not exploit the compositional structure of robotic domains and struggle to generalize to unseen task combinations. We propose a semantic compositional diffusion transformer that factorizes transitions into robot-, object-, obstacle-, and objective-specific components and learns their interactions through attention. Once trained on a limited subset of tasks, we show that our model can zero-shot generate high-quality transitions from which we can learn control policies for unseen task combinations. Then, we introduce an iterative self-improvement procedure in which synthetic data is validated via offline reinforcement learning and incorporated into subsequent training rounds. Our approach substantially improves zero-shot performance over monolithic and hard-coded compositional baselines, ultimately solving nearly all held-out tasks and demonstrating the emergence of meaningful compositional structure in the learned representations.





## MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence
- **Url**: http://arxiv.org/abs/2512.10863v1
- **Authors**: ['Jingli Lin', 'Runsen Xu', 'Shaohao Zhu', 'Sihan Yang', 'Peizhou Cao', 'Yunlong Ran', 'Miao Hu', 'Chenming Zhu', 'Yiman Xie', 'Yilin Long', 'Wenbo Hu', 'Dahua Lin', 'Tai Wang', 'Jiangmiao Pang']
- **Abstrat**: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.





## From Generated Human Videos to Physically Plausible Robot Trajectories
- **Url**: http://arxiv.org/abs/2512.05094v2
- **Authors**: ['James Ni', 'Zekai Wang', 'Wei Lin', 'Amir Bar', 'Yann LeCun', 'Trevor Darrell', 'Jitendra Malik', 'Roei Herzig']
- **Abstrat**: Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.





## Reframing Human-Robot Interaction Through Extended Reality: Unlocking Safer, Smarter, and More Empathic Interactions with Virtual Robots and Foundation Models
- **Url**: http://arxiv.org/abs/2512.02569v2
- **Authors**: ['Yuchong Zhang', 'Yong Ma', 'Danica Kragic']
- **Abstrat**: This perspective reframes human-robot interaction (HRI) through extended reality (XR), arguing that virtual robots powered by large foundation models (FMs) can serve as cognitively grounded, empathic agents. Unlike physical robots, XR-native agents are unbound by hardware constraints and can be instantiated, adapted, and scaled on demand, while still affording embodiment and co-presence. We synthesize work across XR, HRI, and cognitive AI to show how such agents can support safety-critical scenarios, socially and cognitively empathic interaction across domains, and outreaching physical capabilities with XR and AI integration. We then discuss how multimodal large FMs (e.g., large language model, large vision model, and vision-language model) enable context-aware reasoning, affect-sensitive situations, and long-term adaptation, positioning virtual robots as cognitive and empathic mediators rather than mere simulation assets. At the same time, we highlight challenges and potential risks, including overtrust, cultural and representational bias, privacy concerns around biometric sensing, and data governance and transparency. The paper concludes by outlining a research agenda for human-centered, ethically grounded XR agents - emphasizing multi-layered evaluation frameworks, multi-user ecosystems, mixed virtual-physical embodiment, and societal and ethical design practices to envision XR-based virtual agents powered by FMs as reshaping future HRI into a more efficient and adaptive paradigm.





## Panoramic Out-of-Distribution Segmentation
- **Url**: http://arxiv.org/abs/2505.03539v3
- **Authors**: ['Mengfei Duan', 'Yuheng Zhang', 'Yihong Cao', 'Fei Teng', 'Kai Luo', 'Jiaming Zhang', 'Kailun Yang', 'Zhiyong Li']
- **Abstrat**: Panoramic imaging enables capturing 360° images with an ultra-wide Field-of-View (FoV) for dense omnidirectional perception, which is critical to applications, such as autonomous driving and augmented reality, etc. However, current panoramic semantic segmentation methods fail to identify outliers, and pinhole Out-of-distribution Segmentation (OoS) models perform unsatisfactorily in the panoramic domain due to pixel distortions and background clutter. To address these issues, we introduce a new task, Panoramic Out-of-distribution Segmentation (PanOoS), with the aim of achieving comprehensive and safe scene understanding. Furthermore, we propose the first solution, POS, which adapts to the characteristics of panoramic images through text-guided prompt distribution learning. Specifically, POS integrates a disentanglement strategy designed to materialize the cross-domain generalization capability of CLIP. The proposed Prompt-based Restoration Attention (PRA) optimizes semantic decoding by prompt guidance and self-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL) refines the manifold of per-pixel mask embeddings via semantic prototype supervision. Besides, to compensate for the scarcity of PanOoS datasets, we establish two benchmarks: DenseOoS, which features diverse outliers in complex environments, and QuadOoS, captured by a quadruped robot with a panoramic annular lens system. Extensive experiments demonstrate superior performance of POS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS, outperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves leading closed-set segmentation capabilities and advances the development of panoramic understanding. Code and datasets will be available at https://github.com/MengfeiD/PanOoS.





## Evaluating Gemini Robotics Policies in a Veo World Simulator
- **Url**: http://arxiv.org/abs/2512.10675v1
- **Authors**: ['Gemini Robotics Team', 'Coline Devin', 'Yilun Du', 'Debidatta Dwibedi', 'Ruiqi Gao', 'Abhishek Jindal', 'Thomas Kipf', 'Sean Kirmani', 'Fangchen Liu', 'Anirudha Majumdar', 'Andrew Marmon', 'Carolina Parada', 'Yulia Rubanova', 'Dhruv Shah', 'Vikas Sindhwani', 'Jie Tan', 'Fei Xia', 'Ted Xiao', 'Sherry Yang', 'Wenhao Yu', 'Allan Zhou']
- **Abstrat**: Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.





## Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching
- **Url**: http://arxiv.org/abs/2512.10674v1
- **Authors**: ['Javier Villena Toro', 'Mehdi Tarkian']
- **Abstrat**: Recent progress in zero-shot 6D object pose estimation has been driven largely by large-scale models and cloud-based inference. However, these approaches often introduce high latency, elevated energy consumption, and deployment risks related to connectivity, cost, and data governance; factors that conflict with the practical constraints of real-world robotics, where compute is limited and on-device inference is frequently required. We introduce Geo6DPose, a lightweight, fully local, and training-free pipeline for zero-shot 6D pose estimation that trades model scale for geometric reliability. Our method combines foundation model visual features with a geometric filtering strategy: Similarity maps are computed between onboarded template DINO descriptors and scene patches, and mutual correspondences are established by projecting scene patch centers to 3D and template descriptors to the object model coordinate system. Final poses are recovered via correspondence-driven RANSAC and ranked using a weighted geometric alignment metric that jointly accounts for reprojection consistency and spatial support, improving robustness to noise, clutter, and partial visibility. Geo6DPose achieves sub-second inference on a single commodity GPU while matching the average recall of significantly larger zero-shot baselines (53.7 AR, 1.08 FPS). It requires no training, fine-tuning, or network access, and remains compatible with evolving foundation backbones, advancing practical, fully local 6D perception for robotic deployment.





## XDen-1K: A Density Field Dataset of Real-World Objects
- **Url**: http://arxiv.org/abs/2512.10668v1
- **Authors**: ['Jingxuan Zhang', 'Tianqi Yu', 'Yatu Zhang', 'Jinze Wu', 'Kaixin Yao', 'Jingyang Liu', 'Yuyao Zhang', 'Jiayuan Gu', 'Jingyi Yu']
- **Abstrat**: A deep understanding of the physical world is a central goal for embodied AI and realistic simulation. While current models excel at capturing an object's surface geometry and appearance, they largely neglect its internal physical properties. This omission is critical, as properties like volumetric density are fundamental for predicting an object's center of mass, stability, and interaction dynamics in applications ranging from robotic manipulation to physical simulation. The primary bottleneck has been the absence of large-scale, real-world data. To bridge this gap, we introduce XDen-1K, the first large-scale, multi-modal dataset designed for real-world physical property estimation, with a particular focus on volumetric density. The core of this dataset consists of 1,000 real-world objects across 148 categories, for which we provide comprehensive multi-modal data, including a high-resolution 3D geometric model with part-level annotations and a corresponding set of real-world biplanar X-ray scans. Building upon this data, we introduce a novel optimization framework that recovers a high-fidelity volumetric density field of each object from its sparse X-ray views. To demonstrate its practical value, we add X-ray images as a conditioning signal to an existing segmentation network and perform volumetric segmentation. Furthermore, we conduct experiments on downstream robotics tasks. The results show that leveraging the dataset can effectively improve the accuracy of center-of-mass estimation and the success rate of robotic manipulation. We believe XDen-1K will serve as a foundational resource and a challenging new benchmark, catalyzing future research in physically grounded visual inference and embodied AI.





## K-Track: Kalman-Enhanced Tracking for Accelerating Deep Point Trackers on Edge Devices
- **Url**: http://arxiv.org/abs/2512.10628v1
- **Authors**: ['Bishoy Galoaa', 'Pau Closas', 'Sarah Ostadabbas']
- **Abstrat**: Point tracking in video sequences is a foundational capability for real-world computer vision applications, including robotics, autonomous systems, augmented reality, and video analysis. While recent deep learning-based trackers achieve state-of-the-art accuracy on challenging benchmarks, their reliance on per-frame GPU inference poses a major barrier to deployment on resource-constrained edge devices, where compute, power, and connectivity are limited. We introduce K-Track (Kalman-enhanced Tracking), a general-purpose, tracker-agnostic acceleration framework designed to bridge this deployment gap. K-Track reduces inference cost by combining sparse deep learning keyframe updates with lightweight Kalman filtering for intermediate frame prediction, using principled Bayesian uncertainty propagation to maintain temporal coherence. This hybrid strategy enables 5-10X speedup while retaining over 85% of the original trackers' accuracy. We evaluate K-Track across multiple state-of-the-art point trackers and demonstrate real-time performance on edge platforms such as the NVIDIA Jetson Nano and RTX Titan. By preserving accuracy while dramatically lowering computational requirements, K-Track provides a practical path toward deploying high-quality point tracking in real-world, resource-limited settings, closing the gap between modern tracking algorithms and deployable vision systems.





## LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator
- **Url**: http://arxiv.org/abs/2512.10605v1
- **Authors**: ['Lihuang Chen', 'Xiangyu Luo', 'Jun Meng']
- **Abstrat**: We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent.





## Towards Open-World Human Action Segmentation Using Graph Convolutional Networks
- **Url**: http://arxiv.org/abs/2507.00756v2
- **Authors**: ['Hao Xing', 'Kai Zhe Boey', 'Gordon Cheng']
- **Abstrat**: Human-object interaction segmentation is a fundamental task of daily activity understanding, which plays a crucial role in applications such as assistive robotics, healthcare, and autonomous systems. Most existing learning-based methods excel in closed-world action segmentation, they struggle to generalize to open-world scenarios where novel actions emerge. Collecting exhaustive action categories for training is impractical due to the dynamic diversity of human activities, necessitating models that detect and segment out-of-distribution actions without manual annotation. To address this issue, we formally define the open-world action segmentation problem and propose a structured framework for detecting and segmenting unseen actions. Our framework introduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional Network (EPGCN) with a novel decoder module for robust spatiotemporal feature upsampling. 2) Mixup-based training to synthesize out-of-distribution data, eliminating reliance on manual annotations. 3) A novel Temporal Clustering loss that groups in-distribution actions while distancing out-of-distribution samples.   We evaluate our framework on two challenging human-object interaction recognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets. Experimental results demonstrate significant improvements over state-of-the-art action segmentation models across multiple open-set evaluation metrics, achieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and out-of-distribution detection performances (AUROC), respectively. Additionally, we conduct an in-depth ablation study to assess the impact of each proposed component, identifying the optimal framework configuration for open-world action segmentation.





## Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation
- **Url**: http://arxiv.org/abs/2507.00752v2
- **Authors**: ['Hao Xing', 'Kai Zhe Boey', 'Yuankai Wu', 'Darius Burschka', 'Gordon Cheng']
- **Abstrat**: Accurate temporal segmentation of human actions is critical for intelligent robots in collaborative settings, where a precise understanding of sub-activity labels and their temporal structure is essential. However, the inherent noise in both human pose estimation and object detection often leads to over-segmentation errors, disrupting the coherence of action sequences. To address this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that integrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g., 30 fps) motion data (skeleton and object detections) to mitigate fragmentation. Our framework introduces three key contributions. First, a sinusoidal encoding strategy that maps 3D skeleton coordinates into a continuous sin-cos space to enhance spatial representation robustness. Second, a temporal graph fusion module that aligns multi-modal inputs with differing resolutions via hierarchical feature aggregation, Third, inspired by the smooth transitions inherent to human actions, we design SmoothLabelMix, a data augmentation technique that mixes input sequences and labels to generate synthetic training examples with gradual action transitions, enhancing temporal consistency in predictions and reducing over-segmentation artifacts.   Extensive experiments on the Bimanual Actions Dataset, a public benchmark for human-object interaction understanding, demonstrate that our approach outperforms state-of-the-art methods, especially in action segmentation accuracy, achieving F1@10: 94.5% and F1@25: 92.8%.





## Mr. Virgil: Learning Multi-robot Visual-range Relative Localization
- **Url**: http://arxiv.org/abs/2512.10540v1
- **Authors**: ['Si Wang', 'Zhehan Li', 'Jiadong Lu', 'Rong Xiong', 'Yanjun Cao', 'Yue Wang']
- **Abstrat**: Ultra-wideband (UWB)-vision fusion localization has achieved extensive applications in the domain of multi-agent relative localization. The challenging matching problem between robots and visual detection renders existing methods highly dependent on identity-encoded hardware or delicate tuning algorithms. Overconfident yet erroneous matches may bring about irreversible damage to the localization system. To address this issue, we introduce Mr. Virgil, an end-to-end learning multi-robot visual-range relative localization framework, consisting of a graph neural network for data association between UWB rangings and visual detections, and a differentiable pose graph optimization (PGO) back-end. The graph-based front-end supplies robust matching results, accurate initial position predictions, and credible uncertainty estimates, which are subsequently integrated into the PGO back-end to elevate the accuracy of the final pose estimation. Additionally, a decentralized system is implemented for real-world applications. Experiments spanning varying robot numbers, simulation and real-world, occlusion and non-occlusion conditions showcase the stability and exactitude under various scenes compared to conventional methods. Our code is available at: https://github.com/HiOnes/Mr-Virgil.





## Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning
- **Url**: http://arxiv.org/abs/2511.14396v2
- **Authors**: ['Xiuxiu Qi', 'Yu Yang', 'Jiannong Cao', 'Luyao Bai', 'Chongshan Fan', 'Chengtai Cao', 'Hongpeng Wang']
- **Abstrat**: Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.





## Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections
- **Url**: http://arxiv.org/abs/2506.16685v4
- **Authors**: ['Xiaomeng Xu', 'Yifan Hou', 'Zeyi Liu', 'Shuran Song']
- **Abstrat**: We address key challenges in Dataset Aggregation (DAgger) for real-world contact-rich manipulation: how to collect informative human correction data and how to effectively update policies with this new data. We introduce Compliant Residual DAgger (CR-DAgger), which contains two novel components: 1) a Compliant Intervention Interface that leverages compliance control, allowing humans to provide gentle, accurate delta action corrections without interrupting the ongoing robot policy execution; and 2) a Compliant Residual Policy formulation that learns from human corrections while incorporating force feedback and force control. Our system significantly enhances performance on precise contact-rich manipulation tasks using minimal correction data, improving base policy success rates by over 50\% on two challenging tasks (book flipping and belt assembly) while outperforming both retraining-from-scratch and finetuning approaches. Through extensive real-world experiments, we provide practical guidance for implementing effective DAgger in real-world robot learning tasks. Result videos are available at: https://compliant-residual-dagger.github.io/





## UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning
- **Url**: http://arxiv.org/abs/2512.10492v1
- **Authors**: ['Jiaxi Wu', 'Tiantian Zhang', 'Yuxing Wang', 'Yongzhe Chang', 'Xueqian Wang']
- **Abstrat**: Robust adversarial reinforcement learning has emerged as an effective paradigm for training agents to handle uncertain disturbance in real environments, with critical applications in sequential decision-making domains such as autonomous driving and robotic control. Within this paradigm, agent training is typically formulated as a zero-sum Markov game between a protagonist and an adversary to enhance policy robustness. However, the trainable nature of the adversary inevitably induces non-stationarity in the learning dynamics, leading to exacerbated training instability and convergence difficulties, particularly in high-dimensional complex environments. In this paper, we propose a novel approach, Uncertainty-Aware Critic Ensemble for robust adversarial Reinforcement learning (UACER), which consists of two strategies: 1) Diversified critic ensemble: a diverse set of K critic networks is exploited in parallel to stabilize Q-value estimation rather than conventional single-critic architectures for both variance reduction and robustness enhancement. 2) Time-varying Decay Uncertainty (TDU) mechanism: advancing beyond simple linear combinations, we develop a variance-derived Q-value aggregation strategy that explicitly incorporates epistemic uncertainty to dynamically regulate the exploration-exploitation trade-off while simultaneously stabilizing the training process. Comprehensive experiments across several MuJoCo control problems validate the superior effectiveness of UACER, outperforming state-of-the-art methods in terms of overall performance, stability, and efficiency.





## Contact SLAM: An Active Tactile Exploration Policy Based on Physical Reasoning Utilized in Robotic Fine Blind Manipulation Tasks
- **Url**: http://arxiv.org/abs/2512.10481v1
- **Authors**: ['Gaozhao Wang', 'Xing Liu', 'Zhenduo Ye', 'Zhengxiong Liu', 'Panfeng Huang']
- **Abstrat**: Contact-rich manipulation is difficult for robots to execute and requires accurate perception of the environment. In some scenarios, vision is occluded. The robot can then no longer obtain real-time scene state information through visual feedback. This is called ``blind manipulation". In this manuscript, a novel physically-driven contact cognition method, called ``Contact SLAM", is proposed. It estimates the state of the environment and achieves manipulation using only tactile sensing and prior knowledge of the scene. To maximize exploration efficiency, this manuscript also designs an active exploration policy. The policy gradually reduces uncertainties in the manipulation scene. The experimental results demonstrated the effectiveness and accuracy of the proposed method in several contact-rich tasks, including the difficult and delicate socket assembly task and block-pushing task.





## Symphony: A Heuristic Normalized Calibrated Advantage Actor and Critic Algorithm in application for Humanoid Robots
- **Url**: http://arxiv.org/abs/2512.10477v1
- **Authors**: ['Timur Ishuov', 'Michele Folgheraiter', 'Madi Nurmanov', 'Goncalo Gordo', 'Richárd Farkas', 'József Dombi']
- **Abstrat**: In our work we not explicitly hint that it is a misconception to think that humans learn fast. Learning process takes time. Babies start learning to move in the restricted liquid area called placenta. Children often are limited by underdeveloped body. Even adults are not allowed to participate in complex competitions right away. However, with robots, when learning from scratch, we often don't have the privilege of waiting for dozen millions of steps. "Swaddling" regularization is responsible for restraining an agent in rapid but unstable development penalizing action strength in a specific way not affecting actions directly. The Symphony, Transitional-policy Deterministic Actor and Critic algorithm, is a concise combination of different ideas for possibility of training humanoid robots from scratch with Sample Efficiency, Sample Proximity and Safety of Actions in mind. It is no secret that continuous increase in Gaussian noise without appropriate smoothing is harmful for motors and gearboxes. Compared to Stochastic algorithms, we set a limited parametric noise and promote a reduced strength of actions, safely increasing entropy, since the actions are kind of immersed in weaker noise. When actions require more extreme values, actions rise above the weak noise. Training becomes empirically much safer for both the environment around and the robot's mechanisms. We use Fading Replay Buffer: using a fixed formula containing the hyperbolic tangent, we adjust the batch sampling probability: the memory contains a recent memory and a long-term memory trail. Fading Replay Buffer allows us to use Temporal Advantage when we improve the current Critic Network prediction compared to the exponential moving average. Temporal Advantage allows us to update Actor and Critic in one pass, as well as combine Actor and Critic in one Object and implement their Losses in one line.




